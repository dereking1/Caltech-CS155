{"cells":[{"cell_type":"markdown","metadata":{"id":"NG2ZYJ8OQkYu"},"source":["# Problem 3"]},{"cell_type":"markdown","metadata":{"id":"Uswwk6-5QkYz"},"source":["Use this notebook to write your code for problem 3."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"mNDH7qAJQkYz","executionInfo":{"status":"ok","timestamp":1643351699251,"user_tz":480,"elapsed":4,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}}},"outputs":[],"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"]},{"cell_type":"markdown","metadata":{"id":"yXCTagSFQkY2"},"source":["## 3D - Convolutional network"]},{"cell_type":"markdown","metadata":{"id":"FcYqb7ujQkY2"},"source":["As in problem 2, we have conveniently provided for your use code that loads and preprocesses the MNIST data."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"uTdlcHOmQkY3","executionInfo":{"status":"ok","timestamp":1643351700556,"user_tz":480,"elapsed":1307,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}}},"outputs":[],"source":["# load MNIST data into PyTorch format\n","import torch\n","import torchvision\n","import torchvision.transforms as transforms\n","\n","# set batch size\n","batch_size = 32\n","\n","# load training data downloaded into data/ folder\n","mnist_training_data = torchvision.datasets.MNIST('data/', train=True, download=True,\n","                                                transform=transforms.ToTensor())\n","# transforms.ToTensor() converts batch of images to 4-D tensor and normalizes 0-255 to 0-1.0\n","training_data_loader = torch.utils.data.DataLoader(mnist_training_data,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=True)\n","\n","# load test data\n","mnist_test_data = torchvision.datasets.MNIST('data/', train=False, download=True,\n","                                                transform=transforms.ToTensor())\n","test_data_loader = torch.utils.data.DataLoader(mnist_test_data,\n","                                                  batch_size=batch_size,\n","                                                  shuffle=False)"]},{"cell_type":"code","execution_count":4,"metadata":{"id":"Y9RlGiKZQkY4","outputId":"0d1f941f-427f-4c20-d26e-09c558708698","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643351700557,"user_tz":480,"elapsed":27,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["1875 training batches\n","60000 training samples\n","313 validation batches\n"]}],"source":["# look at the number of batches per epoch for training and validation\n","print(f'{len(training_data_loader)} training batches')\n","print(f'{len(training_data_loader) * batch_size} training samples')\n","print(f'{len(test_data_loader)} validation batches')"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"WA8r-bO7QkY5","executionInfo":{"status":"ok","timestamp":1643351700557,"user_tz":480,"elapsed":25,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}}},"outputs":[],"source":["# my model\n","import torch.nn as nn\n","\n","model = nn.Sequential(\n","    nn.Conv2d(1, 8, kernel_size=(3,3)),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.25),\n","    \n","    nn.Conv2d(8, 8, kernel_size=(3,3)),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.25),\n","    \n","    nn.Flatten(),\n","    nn.Linear(25*8, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n","    # PyTorch implementation of cross-entropy loss includes softmax layer\n",")"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"vypV9oSRQkY7","outputId":"628870f6-49c9-4991-e4a9-3bf29a47e757","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643351700558,"user_tz":480,"elapsed":25,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["torch.Size([8, 1, 3, 3])\n","torch.Size([8])\n","torch.Size([8, 8, 3, 3])\n","torch.Size([8])\n","torch.Size([64, 200])\n","torch.Size([64])\n","torch.Size([10, 64])\n","torch.Size([10])\n"]}],"source":["# why don't we take a look at the shape of the weights for each layer\n","for p in model.parameters():\n","    print(p.data.shape)"]},{"cell_type":"code","execution_count":7,"metadata":{"id":"3mHMVuwQQkY7","outputId":"be01d41b-2581-459a-a66d-77876c60e554","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643351700558,"user_tz":480,"elapsed":19,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["total params: 14178\n"]}],"source":["# our model has some # of parameters:\n","count = 0\n","for p in model.parameters():\n","    n_params = np.prod(list(p.data.shape)).item()\n","    count += n_params\n","print(f'total params: {count}')"]},{"cell_type":"code","execution_count":8,"metadata":{"id":"uJhWXEQ0QkY8","executionInfo":{"status":"ok","timestamp":1643351700559,"user_tz":480,"elapsed":11,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}}},"outputs":[],"source":["# For a multi-class classification problem\n","import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters())"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"w-At3b8DQkY8","outputId":"f580e25d-0374-4090-b989-75f75507bd8b","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1643351874964,"user_tz":480,"elapsed":174415,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10:...........\n","\tloss: 0.4470, acc: 0.8573, val loss: 0.1504, val acc: 0.9588\n","Epoch 2/10:...........\n","\tloss: 0.2718, acc: 0.9134, val loss: 0.1142, val acc: 0.9677\n","Epoch 3/10:...........\n","\tloss: 0.2541, acc: 0.9192, val loss: 0.1451, val acc: 0.9543\n","Epoch 4/10:...........\n","\tloss: 0.2455, acc: 0.9230, val loss: 0.1130, val acc: 0.9653\n","Epoch 5/10:...........\n","\tloss: 0.2490, acc: 0.9232, val loss: 0.0955, val acc: 0.9681\n","Epoch 6/10:...........\n","\tloss: 0.2404, acc: 0.9257, val loss: 0.1085, val acc: 0.9667\n","Epoch 7/10:...........\n","\tloss: 0.2417, acc: 0.9250, val loss: 0.1311, val acc: 0.9635\n","Epoch 8/10:...........\n","\tloss: 0.2355, acc: 0.9271, val loss: 0.1154, val acc: 0.9636\n","Epoch 9/10:...........\n","\tloss: 0.2349, acc: 0.9269, val loss: 0.0980, val acc: 0.9692\n","Epoch 10/10:...........\n","\tloss: 0.2320, acc: 0.9287, val loss: 0.0879, val acc: 0.9732\n"]}],"source":["# Train the model for 10 epochs, iterating on the data in batches\n","n_epochs = 10\n","\n","# store metrics\n","training_accuracy_history = np.zeros([n_epochs, 1])\n","training_loss_history = np.zeros([n_epochs, 1])\n","validation_accuracy_history = np.zeros([n_epochs, 1])\n","validation_loss_history = np.zeros([n_epochs, 1])\n","\n","for epoch in range(n_epochs):\n","    print(f'Epoch {epoch+1}/10:', end='')\n","    train_total = 0\n","    train_correct = 0\n","    # train\n","    model.train()\n","    for i, data in enumerate(training_data_loader):\n","        images, labels = data\n","        optimizer.zero_grad()\n","        # forward pass\n","        output = model(images)\n","        # calculate categorical cross entropy loss\n","        loss = criterion(output, labels)\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # track training accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # track training loss\n","        training_loss_history[epoch] += loss.item()\n","        # progress update after 180 batches (~1/10 epoch for batch size 32)\n","        if i % 180 == 0: print('.',end='')\n","    training_loss_history[epoch] /= len(training_data_loader)\n","    training_accuracy_history[epoch] = train_correct / train_total\n","    print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","        \n","    # validate\n","    test_total = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for i, data in enumerate(test_data_loader):\n","            images, labels = data\n","            # forward pass\n","            output = model(images)\n","            # find accuracy\n","            _, predicted = torch.max(output.data, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","            # find loss\n","            loss = criterion(output, labels)\n","            validation_loss_history[epoch] += loss.item()\n","        validation_loss_history[epoch] /= len(test_data_loader)\n","        validation_accuracy_history[epoch] = test_correct / test_total\n","    print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')"]},{"cell_type":"markdown","metadata":{"id":"JPYoKN_8QkY9"},"source":["Above, we output the training loss/accuracy as well as the validation loss and accuracy. Not bad! Let's see if you can do better."]},{"cell_type":"code","source":["ps = np.linspace(0.1, 1, 10)\n","n_ps = len(ps)\n","accuracies = []\n","\n","training_accuracy_history = np.zeros([n_ps, 1])\n","training_loss_history = np.zeros([n_ps, 1])\n","validation_accuracy_history = np.zeros([n_ps, 1])\n","validation_loss_history = np.zeros([n_ps, 1])\n","\n","for j, p in enumerate(ps):\n","\n","    print(f'p = {p:.1f}:')\n","    model = nn.Sequential(\n","        nn.Conv2d(1, 16, kernel_size=(3,3)),\n","        nn.BatchNorm2d(16),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2),\n","        nn.Dropout(p=p),\n","        \n","        nn.Conv2d(16, 8, kernel_size=(3,3)),\n","        nn.BatchNorm2d(8),\n","        nn.ReLU(),\n","        nn.MaxPool2d(2),\n","        nn.Dropout(p=p),\n","\n","        nn.Flatten(),\n","        nn.Linear(25*8, 64),\n","        nn.ReLU(),\n","        nn.Linear(64, 10)\n","        # PyTorch implementation of cross-entropy loss includes softmax layer\n","    )\n","\n","    # For a multi-class classification problem\n","    import torch.optim as optim\n","    criterion = nn.CrossEntropyLoss()\n","    optimizer = optim.RMSprop(model.parameters())\n","\n","    train_total = 0\n","    train_correct = 0\n","    # train\n","    model.train()\n","    for i, data in enumerate(training_data_loader):\n","        images, labels = data\n","        optimizer.zero_grad()\n","        # forward pass\n","        output = model(images)\n","        # calculate categorical cross entropy loss\n","        loss = criterion(output, labels)\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # track training accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # track training loss\n","        training_loss_history[epoch] += loss.item()\n","        # progress update after 180 batches (~1/10 epoch for batch size 32)\n","        if i % 180 == 0: print('.',end='')\n","    training_loss_history[j] /= len(training_data_loader)\n","    training_accuracy_history[j] = train_correct / train_total\n","    print(f'\\n\\tloss: {training_loss_history[j,0]:0.4f}, acc: {training_accuracy_history[j,0]:0.4f}',end='')\n","        \n","    # validate\n","    test_total = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for i, data in enumerate(test_data_loader):\n","            images, labels = data\n","            # forward pass\n","            output = model(images)\n","            # find accuracy\n","            _, predicted = torch.max(output.data, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","            # find loss\n","            loss = criterion(output, labels)\n","            validation_loss_history[epoch] += loss.item()\n","        validation_loss_history[j] /= len(test_data_loader)\n","        validation_accuracy_history[j] = test_correct / test_total\n","    print(f', val loss: {validation_loss_history[j,0]:0.4f}, val acc: {validation_accuracy_history[j,0]:0.4f}')\n","\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GGV9bXb4Woys","executionInfo":{"status":"ok","timestamp":1643355026153,"user_tz":480,"elapsed":285319,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}},"outputId":"47956002-260d-49d2-969e-2fb1d641be25"},"execution_count":52,"outputs":[{"output_type":"stream","name":"stdout","text":["p = 0.1:\n","...........\n","\tloss: 0.0000, acc: 0.9349, val loss: 0.0000, val acc: 0.9816\n","p = 0.2:\n","...........\n","\tloss: 0.0000, acc: 0.9185, val loss: 0.0000, val acc: 0.9684\n","p = 0.3:\n","...........\n","\tloss: 0.0000, acc: 0.9062, val loss: 0.0000, val acc: 0.9678\n","p = 0.4:\n","...........\n","\tloss: 0.0000, acc: 0.8827, val loss: 0.0000, val acc: 0.9686\n","p = 0.5:\n","...........\n","\tloss: 0.0000, acc: 0.8618, val loss: 0.0000, val acc: 0.9568\n","p = 0.6:\n","...........\n","\tloss: 0.0000, acc: 0.7820, val loss: 0.0000, val acc: 0.9541\n","p = 0.7:\n","...........\n","\tloss: 0.0000, acc: 0.7471, val loss: 0.0000, val acc: 0.9327\n","p = 0.8:\n","...........\n","\tloss: 0.0000, acc: 0.5751, val loss: 0.0000, val acc: 0.8740\n","p = 0.9:\n","...........\n","\tloss: 0.0000, acc: 0.3323, val loss: 0.0000, val acc: 0.8033\n","p = 1.0:\n","...........\n","\tloss: 8.5142, acc: 0.1088, val loss: 154.1664, val acc: 0.0971\n"]}]},{"cell_type":"code","source":["# final model\n","model = nn.Sequential(\n","    nn.Conv2d(1, 16, kernel_size=(3,3)),\n","    nn.BatchNorm2d(16),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.1),\n","        \n","    nn.Conv2d(16, 8, kernel_size=(3,3)),\n","    nn.BatchNorm2d(8),\n","    nn.ReLU(),\n","    nn.MaxPool2d(2),\n","    nn.Dropout(p=0.1),\n","\n","    nn.Flatten(),\n","    nn.Linear(25*8, 64),\n","    nn.ReLU(),\n","    nn.Linear(64, 10)\n","    # PyTorch implementation of cross-entropy loss includes softmax layer\n","  )\n","\n","# our model has some # of parameters:\n","count = 0\n","for p in model.parameters():\n","    n_params = np.prod(list(p.data.shape)).item()\n","    count += n_params\n","print(f'total params: {count}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"WtySvZKjdogl","executionInfo":{"status":"ok","timestamp":1643355126947,"user_tz":480,"elapsed":506,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}},"outputId":"2d1b748a-ecce-43a0-c1c3-7cf49e29c635"},"execution_count":55,"outputs":[{"output_type":"stream","name":"stdout","text":["total params: 14882\n"]}]},{"cell_type":"code","source":["# For a multi-class classification problem\n","import torch.optim as optim\n","criterion = nn.CrossEntropyLoss()\n","optimizer = optim.RMSprop(model.parameters())\n","\n","# Train the model for 10 epochs, iterating on the data in batches\n","n_epochs = 10\n","\n","# store metrics\n","training_accuracy_history = np.zeros([n_epochs, 1])\n","training_loss_history = np.zeros([n_epochs, 1])\n","validation_accuracy_history = np.zeros([n_epochs, 1])\n","validation_loss_history = np.zeros([n_epochs, 1])\n","\n","for epoch in range(n_epochs):\n","    print(f'Epoch {epoch+1}/10:', end='')\n","    train_total = 0\n","    train_correct = 0\n","    # train\n","    model.train()\n","    for i, data in enumerate(training_data_loader):\n","        images, labels = data\n","        optimizer.zero_grad()\n","        # forward pass\n","        output = model(images)\n","        # calculate categorical cross entropy loss\n","        loss = criterion(output, labels)\n","        # backward pass\n","        loss.backward()\n","        optimizer.step()\n","        \n","        # track training accuracy\n","        _, predicted = torch.max(output.data, 1)\n","        train_total += labels.size(0)\n","        train_correct += (predicted == labels).sum().item()\n","        # track training loss\n","        training_loss_history[epoch] += loss.item()\n","        # progress update after 180 batches (~1/10 epoch for batch size 32)\n","        if i % 180 == 0: print('.',end='')\n","    training_loss_history[epoch] /= len(training_data_loader)\n","    training_accuracy_history[epoch] = train_correct / train_total\n","    print(f'\\n\\tloss: {training_loss_history[epoch,0]:0.4f}, acc: {training_accuracy_history[epoch,0]:0.4f}',end='')\n","        \n","    # validate\n","    test_total = 0\n","    test_correct = 0\n","    with torch.no_grad():\n","        model.eval()\n","        for i, data in enumerate(test_data_loader):\n","            images, labels = data\n","            # forward pass\n","            output = model(images)\n","            # find accuracy\n","            _, predicted = torch.max(output.data, 1)\n","            test_total += labels.size(0)\n","            test_correct += (predicted == labels).sum().item()\n","            # find loss\n","            loss = criterion(output, labels)\n","            validation_loss_history[epoch] += loss.item()\n","        validation_loss_history[epoch] /= len(test_data_loader)\n","        validation_accuracy_history[epoch] = test_correct / test_total\n","    print(f', val loss: {validation_loss_history[epoch,0]:0.4f}, val acc: {validation_accuracy_history[epoch,0]:0.4f}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HbJwJrIxd5QA","executionInfo":{"status":"ok","timestamp":1643355423391,"user_tz":480,"elapsed":292264,"user":{"displayName":"Derek Ing","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjr5IvDjbpOCXH2MZ1eNNPDXrv5whJGnLakLcn5ig=s64","userId":"00616330678310679580"}},"outputId":"007ffd9d-fd67-4dd5-8d3f-55254347bf04"},"execution_count":56,"outputs":[{"output_type":"stream","name":"stdout","text":["Epoch 1/10:...........\n","\tloss: 0.2832, acc: 0.9136, val loss: 0.0986, val acc: 0.9696\n","Epoch 2/10:...........\n","\tloss: 0.1126, acc: 0.9650, val loss: 0.0694, val acc: 0.9782\n","Epoch 3/10:...........\n","\tloss: 0.0993, acc: 0.9701, val loss: 0.0635, val acc: 0.9780\n","Epoch 4/10:...........\n","\tloss: 0.0874, acc: 0.9727, val loss: 0.0698, val acc: 0.9799\n","Epoch 5/10:...........\n","\tloss: 0.0823, acc: 0.9744, val loss: 0.0560, val acc: 0.9812\n","Epoch 6/10:...........\n","\tloss: 0.0763, acc: 0.9769, val loss: 0.0567, val acc: 0.9838\n","Epoch 7/10:...........\n","\tloss: 0.0720, acc: 0.9778, val loss: 0.0534, val acc: 0.9838\n","Epoch 8/10:...........\n","\tloss: 0.0715, acc: 0.9790, val loss: 0.0565, val acc: 0.9824\n","Epoch 9/10:...........\n","\tloss: 0.0693, acc: 0.9792, val loss: 0.0496, val acc: 0.9845\n","Epoch 10/10:...........\n","\tloss: 0.0686, acc: 0.9791, val loss: 0.0488, val acc: 0.9856\n"]}]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"name":"ing_derek_set4_prob3.ipynb","provenance":[{"file_id":"https://github.com/charlesincharge/Caltech-CS155-2022/blob/main/sets/set4/set4_prob3.ipynb","timestamp":1643349616314}],"collapsed_sections":[]}},"nbformat":4,"nbformat_minor":0}